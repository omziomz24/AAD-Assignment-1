---
title: "AAD Assignment 1 - Group 26"
author: "Omar, Eloise, Alina, Sue"
date: "`r Sys.Date()`"
output: pdf_document
---
Testing again
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#2.1 Descriptive analysis of the data set
ALWAYS RUN THIS ONE
Load in data and clean
```{r }
data <- read.csv("Assignt1_data.csv")

# Summary stuff

head(data)
dim(data) 
data$oceanProximity <- as.factor(data$oceanProximity)

# NA's
incomplete_rows <- data[!complete.cases(data), ] # these are all missing aveBedrooms. Maybe just don't use this feature
data <- na.omit(data) 
rownames(data) <- 1:nrow(data) 

```

# Exploratory analysis
``` {r}
#censoring

#make a comment from here:
summary(data)

#other things are also censored

data[order(-data$medianHouseValue), ]
value <- 500001  # change to your target value
percentage <- mean(data$medianHouseValue == value) * 100
cat("Percentage of data with medianHouseValue =", value, "is", percentage, "%\n")

# Probably not super necessary to include
# initial plots
target_col <- "medianHouseValue"
exclude <- c("id","oceanProximity",target_col)

for (col in names(data)) {
  if (!(col %in% exclude)) {
    plot(data[[col]], data[[target_col]],
         xlab = col,
         ylab = target_col,
         main = paste(col, "vs", target_col))
  }
}
# For qualitative variable
boxplot(medianHouseValue ~ oceanProximity, data = data,
        main = "Median House Value by Ocean Proximity",
        xlab = "Ocean Proximity",
        ylab = "Median House Value",
        col = "lightblue")

# Need to include more stuff, very basic here

```

# More exploring
Single linear regression

```{r }

## MOST IMPORTANT BITS

#looks way better
#population, issues with leverage, heterscedascity
lm.fit <- lm(medianHouseValue~population,data = data) 
summary(lm.fit)
plot(lm.fit)

lm.fit <- lm(medianHouseValue~log(population),data = data) 
summary(lm.fit)
plot(lm.fit)



# Point out cluster around LA and SF

plot(data$longitude, data$latitude, 
     xlab = "Longitude", ylab = "Latitude", 
     main = "California Housing Data with LA and SF", 
     pch = 20, col = "grey")

# Add LA point
points(-118.24, 34.05, col = "red", pch = 19, cex = 1.5)
text(-118.24, 34.05, labels = "LA", pos = 3, col = "red")

# Add SF point
points(-122.42, 37.77, col = "blue", pch = 19, cex = 1.5)
text(-122.42, 37.77, labels = "SF", pos = 3, col = "blue")


```

2.2 Multiple linear regression
MUST RUN THIS
```{r }
# initial model
#reinitalise data just in case
data <- read.csv("Assignt1_data.csv")
data <- na.omit(data) 
data$oceanProximity <- as.factor(data$oceanProximity)

lm.fit.og <- lm(medianHouseValue~.,data = data) # ID not a relevant predictor
summary(lm.fit.og)
# population (population in particular) and island and near bay look iffy
deviance(lm.fit.og)
contrasts(data$oceanProximity)


plot(lm.fit.og)
# some outliers/leverage points
# clear heteroscedasticity


# Checking Collinearity
library(car)
vif(lm.fit.og)

# Correlation matrix
numeric_vars <- data[sapply(data, is.numeric)]
cor_matrix <- cor(numeric_vars, use = "complete.obs")
round(cor_matrix, 2)

#ideas for model selection
regfit=regsubsets(medianHouseValue~.,data = data,nvmax =length(coef(lm.fit.og)))
#summary(regfit.full)



```
# New fit extra predictors
``` {r}
# New fit
data$bedroomsPerRoom <- data$aveBedrooms / data$aveRooms
#distance to centre
center_lat <- mean(data$latitude)
center_lon <- mean(data$longitude)

data$distToCenter <- sqrt((data$latitude - center_lat)^2 + (data$longitude - center_lon)^2)

data$incomePerRoom = data$medianIncome / data$aveRooms

# Dist from LA and SF
library(geosphere)
# Need to reference this
la_coords <- c(-118.24, 34.05)   # (longitude, latitude)
sf_coords <- c(-122.42, 37.77)

# Add distance to LA
data$distToLA <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000  # convert meters to km
})

#Add distance to SF
data$distToSF <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
})


# Compute direction angles
data$dirToLA <- atan2(data$latitude - la_coords[2], data$longitude - la_coords[1])
data$dirToSF <- atan2(data$latitude - sf_coords[2], data$longitude - sf_coords[1])

# Encode directions using sin and cos
data$cosDirToLA <- cos(data$dirToLA)
data$sinDirToLA <- sin(data$dirToLA)

data$cosDirToSF <- cos(data$dirToSF)
data$sinDirToSF <- sin(data$dirToSF)

# Remove intermediate angle variables
data$dirToLA <- NULL
data$dirToSF <- NULL

data$cityProximityScore <- 1 / (1 + data$distToLA) + 1 / (1 + data$distToSF)

data$cityProximityScore <- 1 / (1 + data$distToLA) + 1 / (1 + data$distToSF)
data$isCensored <- as.integer(data$medianHouseValue >= 500001)

```



New function - Must run to run next chunk
``` {r}
library(leaps)
#since can't call predict for regsubsets. doing the same thing as the for loop
predict.regsubsets=function(object,newdata,id,...){ #... allows for other arguments to be passed into the function
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

```

# Choosing the best subset for new linear model
```{r, echo=FALSE}

library(car)
# k-fold CV


lm.fit <- lm(medianHouseValue~.-id-oceanProximity-aveBedrooms-aveRooms-population+bedroomsPerRoom+longitude:latitude+log(population)+I(medianIncome^2)+I(medianIncome^3)+I(housingMedianAge^2)+medianIncome:log(population)+bedroomsPerRoom:medianIncome+medianIncome:housingMedianAge+distToCenter:medianIncome+latitude:incomePerRoom+bedroomsPerRoom:distToLA+I(medianIncome/housingMedianAge)+I(latitude^2)+I(longitude^2)+I(latitude^3)+I(longitude^3)+log(incomePerRoom)+log(medianIncome/distToCenter)+housingMedianAge:incomePerRoom+cityProximityScore:housingMedianAge+cityProximityScore:bedroomsPerRoom+medianIncome:cosDirToLA+medianIncome:sinDirToLA+medianIncome:cosDirToSF+medianIncome:sinDirToSF+medianIncome:isCensored,data = data)
summary(lm.fit)
nvmax <- length(coef(lm.fit)) - 1


k=10
set.seed(3)
folds=sample(1:k,nrow(data),replace=TRUE)
cv.errors=matrix(NA,k,nvmax, dimnames=list(NULL, paste(1:nvmax))) # NA means no data, NULL means no row names


for(j in 1:k){
  best.fit=regsubsets(medianHouseValue~.-id-oceanProximity-aveBedrooms-aveRooms-population+bedroomsPerRoom+longitude:latitude+log(population)+I(medianIncome^2)+I(medianIncome^3)+I(housingMedianAge^2)+medianIncome:log(population)+bedroomsPerRoom:medianIncome+medianIncome:housingMedianAge+distToCenter:medianIncome+latitude:incomePerRoom+bedroomsPerRoom:distToLA+I(medianIncome/housingMedianAge)+I(latitude^2)+I(longitude^2)+I(latitude^3)+I(longitude^3)+log(incomePerRoom)+log(medianIncome/distToCenter)+housingMedianAge:incomePerRoom+cityProximityScore:housingMedianAge+cityProximityScore:bedroomsPerRoom+medianIncome:cosDirToLA+medianIncome:sinDirToLA+medianIncome:cosDirToSF+medianIncome:sinDirToSF+medianIncome:isCensored,data = data[folds!=j,],nvmax=nvmax)
  for(i in 1:nvmax){
    pred=predict(best.fit,data[folds==j,],id=i)
    cv.errors[j,i]=mean( (data$medianHouseValue[folds==j]-pred)^2)
  }
}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
best.model.size <- as.numeric(names(which.min(mean.cv.errors)))
plot(mean.cv.errors,type='b')
points(best.model.size, mean.cv.errors[best.model.size], col = "red", pch = 19, cex = 1.5)
# Obtain the best subset model using the full data and CV selected id
reg.best=regsubsets(medianHouseValue~.-id-oceanProximity-aveBedrooms-aveRooms-population+bedroomsPerRoom+longitude:latitude+log(population)+I(medianIncome^2)+I(medianIncome^3)+I(housingMedianAge^2)+medianIncome:log(population)+bedroomsPerRoom:medianIncome+medianIncome:housingMedianAge+distToCenter:medianIncome+latitude:incomePerRoom+bedroomsPerRoom:distToLA+I(medianIncome/housingMedianAge)+I(latitude^2)+I(longitude^2)+I(latitude^3)+I(longitude^3)+log(incomePerRoom)+log(medianIncome/distToCenter)+housingMedianAge:incomePerRoom+cityProximityScore:housingMedianAge+cityProximityScore:bedroomsPerRoom+medianIncome:cosDirToLA+medianIncome:sinDirToLA+medianIncome:cosDirToSF+medianIncome:sinDirToSF+medianIncome:isCensored,data = data, nvmax=nvmax)
coef(reg.best,best.model.size)


best.predictors = names(coef(reg.best, best.model.size))[-1]  # remove intercept
# Create formula dynamically
formula.best = as.formula(paste("medianHouseValue ~", paste(best.predictors, collapse = " + ")))

# Fit the model using lm
model.best = lm(formula.best, data = data)
summary(model.best)
vif(model.best,type = "predictor")
regfit <- regsubsets(formula.best,data = data,nvmax =length(coef(model.best)))
```


# Adding in oceanProximity
``` {r}
# With oceanProximity
# best so far
#oceanProximity+oceanProximity:medianIncome+housingMedianAge:oceanProximity+log(population):oceanProximity+latitude
new.formula = update(formula.best, . ~ . + oceanProximity +oceanProximity:medianIncome + oceanProximity:distToCenter + oceanProximity:incomePerRoom)
model.best = lm(new.formula, data = data)

summary(model.best)
#vif(model.best,type = "predictor")


#with oceanProximity is better

plot(model.best)

names(coef(model.best))
# Remove outliers
data_unlev <- data[-c(7418,7420),] # these are all island - ridge or lasso can get rid of this

#data[data$oceanProximity == "ISLAND",]
#regfit.full=regsubsets(new.formula,data = data,nvmax =length(coef(model.best)))
#summary(regfit.full)
```




#Ridge regression

```{r}

library(glmnet)
x = model.matrix(new.formula, data = data)[, -1]  # Remove intercept column
y = data$medianHouseValue

# Perform 10-fold cross-validation to find the best lambda
set.seed(7)
lambda_grid <- 10^seq(-5, -2, length = 100)  # from 1e-5 to 1e-2
cv.ridge = cv.glmnet(x, y, alpha = 0,lambda = lambda_grid, nfolds = 10,standardize = TRUE)

# Extract best lambda (lambda.min gives lambda with minimum MSE)
best.lambda = cv.ridge$lambda.min

ridge.model = glmnet(x, y, alpha = 0, lambda = best.lambda)
coef(ridge.model)




```



``` {r}

test_data <- read.csv("Assignt1_test full.csv")
test_data$bedroomsPerRoom <- test_data $aveBedrooms / test_data $aveRooms
test_data$distToCenter <- sqrt((test_data$latitude - center_lat)^2 + (test_data$longitude - center_lon)^2)
test_data$incomePerRoom = test_data$medianIncome / test_data$aveRoom
# Add distance to LA
test_data$distToLA <- apply(test_data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000  # convert meters to km
})

#Add distance to SF
test_data$distToSF <- apply(test_data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
})

# Compute direction angles
test_data$dirToLA <- atan2(test_data$latitude - la_coords[2], test_data$longitude - la_coords[1])
test_data$dirToSF <- atan2(test_data$latitude - sf_coords[2], test_data$longitude - sf_coords[1])


# Encode directions using sin and cos
test_data$cosDirToLA <- cos(test_data$dirToLA)
test_data$sinDirToLA <- sin(test_data$dirToLA)

test_data$cosDirToSF <- cos(test_data$dirToSF)
test_data$sinDirToSF <- sin(test_data$dirToSF)

# Remove intermediate angle variables
test_data$dirToLA <- NULL
test_data$dirToSF <- NULL

test_data$cityProximityScore <- 1 / (1 + test_data$distToLA) + 1 / (1 + test_data$distToSF)

test_data$isCensored <- as.integer(test_data$medianHouseValue >= 500001)
actual <- test_data$medianHouseValue

#OG
pred <- predict(lm.fit.og, newdata = test_data  )
pred <- pmax(pmin(pred,500001),14999)
og_mse <- mean((pred - actual)^2)
og_mse


#Best model
pred <- predict(model.best, newdata = test_data  )
pred <- pmax(pmin(pred,500001),14999)
best_mse <- mean((pred - actual)^2)
best_mse


#ridge
test_matrix <- model.matrix(new.formula, data = test_data)[, -1]
pred <- predict(ridge.model, s = best.lambda, newx = test_matrix)
pred <- pmax(pmin(pred,500001),14999)
ridge_mse <- mean((pred - actual)^2)
ridge_mse


```


# Best so far with censoring
[1] 4894747643
[1] 2911083066
[1] 3001812504










