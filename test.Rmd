---
title: "AAD Assignment 1 - Group 26"
author: "Omar, Eloise, Alina, Sue"
date: "`r Sys.Date()`"
output: pdf_document
---

Content:  

1 Descriptive analysis of the data set  1.1 Data loading and cleaning   1.2 Preliminary analysis on the data  

2 Multiple linear regression 2.1 Initial MLR model using all appropriate predictors   2.2 Discussion of the initial model   2.3 Checking issues in the initial model   2.3.1   2.3.2   2.3.3   2.3.4 ....   2.4 Model improvements   2.5 Three most significant variables  

3 Model Performance: initial model vs improved model   3.1 Training MSE   3.1.1 Training MSE of the initial model   3.1.2 Training MSE of the improved model   3.2 Estimating testing error using the 80-20 split validation set approach   3.3 Estimating testing error using 5-fold cross validation   3.4 Estimating testing error using LOOCV   3.5 Conclusion: whether the final MLR model better than the initial  

4 Prediction competition  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# load relevant libraries: 
library(car)
library(dplyr)
library(ggplot2)
library(car)
library(corrplot) # for correlation plot 
library(leaps) # for Best Subset Selection 
# install.packages("stargazer") 
library(stargazer) # for data presentation in 1.1 
```

------------------------------------------------------------------------

# 1 Descriptive analysis of the data set

## 1.1 Data loading and cleaning

There are 18640 data points in the original data set. There are 10 features being observed and/or recorded, inclduing the response variable, Median House Value ("medianHouseValue"). We also present the five-point summary for the numeric variables: 

```{r}
data <- read.csv("Assignt1_data.csv") 
dim(data) 
stargazer(data[-1], summary = TRUE, type = "text")
# str(data) 
# summary(data) --> don't think we should present this one 
# head(data)
# data$oceanProximity <- as.factor(data$oceanProximity)
```

When inspecting the orginal data, we observe that there are 190 rows that have missing values (NA's) in the varaible "aveBedrooms". Since using data points with missing values can lead to biased results, and since 190 out of 18,640 data points is an imaterial proportion, we decided to remove these rows that contains NA's from the original data frame.
```{r }
incomplete_rows <- data[!complete.cases(data), ] 
data <- na.omit(data) 
rownames(data) <- 1:nrow(data) 
```


## 1.2 Preliminary analysis on the data

### 1.2.1 Histogram on the response variable and data censoring
```{r}
hist(data$medianHouseValue,
     xlab = "Median House Value ($)",
     ylab = "Frequency",
     main = "Histogram of Median House Value",
     breaks = 25)
```
From the histogram, it is evident that the medianHouseValue exhibits right-skewness. Interestingly, there are visibly a concentration of values at the right-end, where the bin for the house value of \$500,000 is disproportionately tall. This is likely due to data censoring, where all the medianHouseValue over a certain threshold is recorded at a capped value around \$500,000.

Through further inspection we find out this cap for censoring is at \$500,001.
```{r}
value <- max(data$medianHouseValue)  
percentage <- mean(data$medianHouseValue == value) * 100
cat("Percentage of data with medianHouseValue =", value, "is", percentage, "%\n")
summary(data$medianHouseValue)
```
Ultimately, given the max. value is 500,001 and the fact that the frequency of this exact value is disproportionately high (4.7%), we are led to conclude that 500,001 is the censoring cap applied. This is likely due to the data collection methods used, where houses valued above \$500,000 were simply reported as exactly $500,001. The impact of this is that the data may under-represent the true values and variation in higher-value homes. Moving forward, we have decided to keep these censored observations in but will maintain caution - especially in our interpretation of correlations, and coming to terms with the fact that our model will not be able to provide reliable predictions for values above 500,001.

```{r}
summary(data$medianIncome)
value <- max(data$medianIncome)  
percentage <- mean(data$medianIncome == value) * 100
cat("Percentage of data with medianIncome =", value, "is", percentage, "%\n")
```
It's clear that medianIncome is also censored. Both medianIncome and medianHouseValue are both left censored and right censored, since it is more significant, for this report we will focus on right censoring.medianHouseValue is also more significant so we can focus on that.


### 1.2.2 Correlations among variables
```{r}
numeric.data <-  data[sapply(data, is.numeric)]
head(numeric.data)
cor_medianHouseValue <- cor(data$medianHouseValue, 
                            data[, c("longitude", 
                                            "latitude", 
                                            "housingMedianAge",
                                            "aveRooms",
                                            "aveBedrooms",
                                            "population",
                                            "medianIncome")])
print(cor_medianHouseValue)
corrplot(cor(data[, sapply(data, is.numeric)]), method = "circle")
```
From the correlation matrix plot we can see that among all the predictors, medianHouseValue has highest correlation with Median Income. This is pretty intuitive, as we expect high-income households purchasing more expensive houses. 

Longitude and Latitude are highly negatively correlated, while Average Rooms and Average Bedrooms are highly positively correlated. This multicollinearity can lead to issues such as unstable coefficient estimates and inflated standard errors, as the model struggles to distinguish the individual effects of highly correlated predictors. Consequently, the interpretability of the regression results is compromised. We will address this issue later in section 2.4 Model improvements.

THe rest correlations between the remaining variables with medianHouseValue have a magnitude below 0.3. However, they may still be very useful in predicting the response variable. But we do not give detailed discussion here. 

Note that ID is just for labeling purposes and it is not a relevant predictor, so we don't care about the correlation between ID and other variables.


### 1.2.3 Geospacial plot of medianHouseValue

Housing values can vary based on their proximity to major cities. By analyzing the provided longitude and latitude, we can deduce that the dataset originates from California. To visualize the relative variation in median house values, we create a heatmap that highlights the influence of major cities, specifically Los Angeles (LA) and San Francisco (SF).
```{r}
cities <- data.frame(
  city = c("LA", "SF"), 
  longitude = c(-118.24, -122.42),
  latitude = c(34.05, 37.77)
)

ggplot(data, 
       aes(x = longitude, 
           y = latitude, 
           colour = medianHouseValue)) + 
  geom_point(size = 1) + 
  scale_colour_gradient(low = "blue", high = "yellow") + 
  geom_point(data = cities, 
             aes(x = longitude, y = latitude),
             colour = "red", 
             size = 2) + 
  geom_text(data = cities, 
            aes(label = city), 
            vjust = -1, 
            colour = "red", 
            size = 7, 
            fontface = "bold") + 
  labs(title = "Geospacial Plot of Median House Value", 
       x = "Longitude", 
       y = "Latitude", 
       colour = "House Value") + 
  theme_minimal() 
```
Clearly, median housing value is higher in the SF and LA regions. Therefore, it is reasonable to integrate a region's closeness to these major cities in our regression model later. They are also an appropriate proxy for censoring.

### 1.2.4 Plots of the response variable vs different features

```{r}
target_col <- "medianHouseValue"
exclude <- c("id","oceanProximity",target_col)

for (col in names(data)) {
  if (!(col %in% exclude)) {
    plot(data[[col]], data[[target_col]],
         xlab = col,
         ylab = target_col,
         main = paste(col, "vs", target_col))
  }
}
```
Median House Value vs longitude/latitude: there is no clear linear relationship or overall trend. But we can see that for some specific longtitude/latitude the housing value is higher. This is consistent with our findings in the geospacial plot - the peaks in the above plot has to do with the relative location to major cities, SF and LA. This motivates us to do some transformation on the raw longitude and latitude data while integrating relevant information in the MLR framwork later on.  

Median House Value vs Housing Media Age: No clear pattern again just by looking at the plot. However the high density of data on the rightmost band (representing housingMedianAge = 52) shows another sign of data censoring.  

```{r}
# Should we truncate the age groups? 
```

Median House Value vs Average Rooms / Average Bedrooms: The linear pattern is not clear when we plot against the entire data range for rooms. These two plots exposes some issue of high leverage points - some regions have average number of bedrooms over 30 and average number of rooms over 140. These abnormalities are worthy to be investigated and treated when we fit the MLR.  

```{r}
# Should we comment further on this in the report? Even if zoom in the trend not clear 
# Focus on the range of [0, 20] for aveRooms 
plot(data$aveBedrooms, data$medianHouseValue, 
     xlim = c(0, 20), 
     pch = 16, 
     xlab = "Average Number of Bedrooms (aveRooms)", 
     ylab = "Median House Value (medianHouseValue)", 
     cex = 0.3) 
```

Median House Value vs Population: The relationship appear non-linear even if we zoom in to the population data range 0 to 5000:  

```{r}
plot(data$population, data$medianHouseValue, 
    xlim = c(0, 5000), 
     pch = 16, 
     xlab = "Population", 
     ylab = "Median House Value (medianHouseValue)", 
     cex = 0.3) 
```

Median House Value vs Median Income: We can see some clear positive trend. This resonates with the correlation plot result, where these two features are positively correlated.  

For those pairs where linear relationship is unclear, we will explore log transformations and polynomials in section 1.2.6 single linear regression.

### 1.2.5 Boxplot of Median House Value by Ocean Proximity

For the qualitative variable, Ocean Proximity, we can examine the differences across different classes via a boxplot:

```{r}
boxplot(medianHouseValue ~ oceanProximity, data = data,
        main = "Boxplot of Median House Value by Ocean Proximity",
        xlab = "Ocean Proximity",
        ylab = "Median House Value",
        col = "lightblue")
```

From the plot, island housing appears to be most valuable on average, whereas inland housing appears to be least valuable. The median value for other 3 classes (\<1h ocean, near bay and near ocean) seems to be close to each other.

### 1.2.6 More exploring: key results from single linear regression (Feel like this can be put into 2.3 as well?)

individual pair of relationships are all statistically significant:

But some assumptions necessary for OLS are clearly violated (e.g. heteroskedasiticty, )

Some relationships are clearly non-linear so have to consider transformations 

```{r }
# comment: I don't think we should report all of them below, just the important ones should suffice 
# comment: I will keep the entire, original chunk here for now: 

#Do each linear regression separately and view plots
#longitude, clearly non-linear poly?
lm.fit <- lm(medianHouseValue~longitude,data = data) 
summary(lm.fit)
plot(lm.fit)

# latitude, again non-linear poly?
lm.fit <- lm(medianHouseValue~latitude,data = data) 
summary(lm.fit)
plot(lm.fit)

#housing median age - good fit for first graph
lm.fit <- lm(medianHouseValue~housingMedianAge,data = data) 
summary(lm.fit)
plot(lm.fit)

#aveRooms, definitely non-linear, big issue with high leverage points
lm.fit <- lm(medianHouseValue~aveRooms,data = data) 
summary(lm.fit)
plot(lm.fit)


#aveBedrooms, issue with high leverage points
lm.fit <- lm(medianHouseValue~aveBedrooms,data = data) 
summary(lm.fit)
plot(lm.fit)

#population, issues with leverage, heterscedascity
lm.fit <- lm(medianHouseValue~population,data = data) 
summary(lm.fit)
plot(lm.fit)

#medianIncome non-linear and heterscedascity, some influential points too
lm.fit <- lm(medianHouseValue~medianIncome,data = data) 
summary(lm.fit)
plot(lm.fit)

#oceanProximity catergorical and need to play around with this a bit
lm.fit <- lm(medianHouseValue~oceanProximity,data = data) 
summary(lm.fit)
plot(lm.fit)

# Keep medianIncome the same
#playing around a bit
#medianIncome non-linear and heterscedascity, some influential points too
#better
lm.fit <- lm(medianHouseValue~log(medianIncome),data = data) 
summary(lm.fit)
plot(lm.fit)

#poly 2 looks pretty good, better than log
lm.fit <- lm(medianHouseValue~poly(medianIncome,2),data = data) 
summary(lm.fit)
plot(lm.fit)
```


```{r}
## MOST IMPORTANT BITS

#looks way better
#population, issues with leverage, heterscedascity
lm.fit <- lm(medianHouseValue~log(population),data = data) 
summary(lm.fit)
plot(lm.fit)


lm.fit <- lm(medianHouseValue~bedroomsPerRoom,data = data) 
summary(lm.fit)
plot(lm.fit)

lm.fit <- lm(medianHouseValue~aveBedroomsPerRoom,data = data) 
summary(lm.fit)
plot(lm.fit)
```




------------------------------------------------------------------------------------------------------------------------

# 2 Multiple linear regression 

## 2.1 Initial MLR model using all appropriate predictors

We will be using the all the given predictors except for id, since it is irrelevant.
```{r}
initial_fit <- lm(data$medianHouseValue ~. - id, data = data) 
summary(initial_fit)
```

## 2.2 Discussion of the initial model

The p-value of the F-test is practically zero, therefore we have sufficient evidence to reject the null hypothesis that all regression coefficients are zero. This means that the model has overall significance, and that at least one of the predictors are useful in explaining the variation in medianHouseValue. 

Having concluded that this model (initial_fit) has overall significance, we can gauge the significance of individual predictors through the t-test p-values: At 5% level of significance, we cannot say that population is useful, as we do not have sufficient evidence to reject the null hypothesis that its corresponding coefficient is non-zero. The dummy variable, "NEAR BAY", associated to the categorical predictor, "oceanProximity", is also shown to be insignificant. This might be an indication that comparing to the base case (\<1H OCEAN), NEAR BAY does not lead to significant change in Median House Price.


Having concluded that this model (initial_fit) has overall significance, we can gauge the significance of individual predictors through the t-test p-values: At 5% level of significance, we cannot say that population is useful, as we do not have sufficient evidence to reject the null hypothesis that its corresponding coefficient is non-zero. The dummy variable, "NEAR BAY", associated to the categorical predictor, "oceanProximity", is also shown to be insignificant. This might be an indication that comparing to the base case (<1H OCEAN), NEAR BAY does not lead to significant change in Median House Price. A potential real-world explanation of why nearbay might not be influential is that “bay” is a very broad concept; some could be more favourable than others (eg. close to amenities OR prone to environmental issues) so its hard to deduce a clear relationship between proximity to bay and house value. 

Moreover, the R-squared and adjusted R-squared both equal to 0.617 when corrected to 3 decimal places. This means that 61.7% of the variation in the predictors are useful in explaining the variation in the response variable (medianHouseValue). 

Additionally, this initial model also reveals some clear relationships among the varaibles: 
\begin{itemize}
  \item Median income has a large, positive coefficient so its a strong positive predictor as we anticipated from the preliminary analysis. 
  \item longitude and latitude both have large negative coefficients which reiterates our findings before regarding geographic (major cities) relevance. 
\end{itemize}

 
## 2.3 Checking issues in the initial model

```{r}
par(mfrow = c(2,2))
plot(initial_fit, cex = 0.1)
```

### 2.3.1 Residual Plot:

The residual plot shows that the points are not quite randomly scattered around zero. This means that the homoskedasiticity and linear assumption might be questionable. To address non-linearity, we can consider polynomial terms or interactions.

The funnel shape (the spread of residuals seems to increase as the fitted values increase) within the fitted value range (0, 400000) strengthens my doubt of heteroskedasticity. We may consider transforming some of our predictors later.

There are some outliers in the residuals that are far away from zero. These influential points may be high-leverage or outliers or both - should be investigated later.

```{r}
par(mfrow = c(1, 1)) 
options(scipen = 999) 
plot(initial_fit, which = 1, cex = 0.2, pch = 16, cex.axis = 0.6,
     main = "Residual Plot (Residual vs Fitted) of Fit1 (using all Predictors)") 
```

### 2.3.2 QQ Plot for Residuals:

A key assumption behind generalized linear model is that the error term is normally distributed. This is why t-statistics and F-statistics can be used in our previous testing.

T-stats are robust under some mild deviation from normality, but under extreme non-normality, these statistics become less reliable.

In our plot, the points deviates from the reference line (dashed line) for larger and smaller quantiles (roughly outside of this range: (-2, 1.5)), indicating non-normality (especially high skewness) and influence of outliers especially at the tails.

```{r}
plot(initial_fit, which = 2, cex = 0.2, pch = 16, cex.axis = 0.8,
     main = "(Normal) QQ Plot of Fit1 (using all Predictors)") 
```

### 2.3.3 Outliers

Outlier identification: as a rule of thumb, we consider those whose studentised residual has a magnitude greater than 3 as outliers here.

However we cannot just simply remove the outliers in this case. This is because these outliers could be attributable to model specification or other problems. We will see what we can do with the model selection and then can come back to this later.

```{r}
residuals_initial_fit <- residuals(initial_fit)
stdresiduals_initial_fit <- rstandard(initial_fit)
outlier_row_number <- which(abs(stdresiduals_initial_fit) > 3)

length(outlier_row_number) # gives how many outliers are there 
```

### 2.3.4 High Leverage Points

We will compute the leverage statistic hi and to see whether it is \>\> (p + 1)/n. Where p is the number of predictors in the model and n is the sample size.

Having 3,317 high leverage points out of 18,450 data points means that about 18% of the data has high leverage. This indicates that our regression line can change dramatically with small changes in the predictors. One possible reason is that we are overfitting the data - and a reason to this is having too many predictor variables.

```{r}
leverage_values <- hatvalues(initial_fit)

p_lvg <- length(coef(initial_fit)) 
n_lvg <- nrow(data) 
threshold_lvg <- (p_lvg + 1) / n_lvg

highlvg_row_number <- which(leverage_values > threshold_lvg)
# highlvg_row_number 
length(highlvg_row_number)

plot(leverage_values, type = "h", col = "blue")
abline(h = 2 * mean(leverage_values), col = "red")
```

### 2.3.5 Collinearity

The arbitrary threshold of severe collinearity is VIF greater or equal to 5. Here, all the predictors are shown to have a non-severe VIF. However, Longitude and Latitude shows relatively high VIF comparing to other predictors - a cause of this is the high correlation between the two.

```{r}
vif(initial_fit)
```

## 2.4 Model Improvements

### 2.4.1 New predictors added

```{r }

# New fit
data$bedroomsPerRoom <- data$aveBedrooms / data$aveRooms
#distance to centre
center_lat <- mean(data$latitude)
center_lon <- mean(data$longitude)

data$distToCenter <- sqrt((data$latitude - center_lat)^2 + (data$longitude - center_lon)^2)

data$incomePerRoom = data$medianIncome / data$aveRooms

# Dist from LA and SF
library(geosphere)
# Need to reference this
la_coords <- c(-118.24, 34.05)   # (longitude, latitude)
sf_coords <- c(-122.42, 37.77)

# Add distance to LA
data$distToLA <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000  # convert meters to km
})

#Add distance to SF
data$distToSF <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
})


# Compute direction angles
data$dirToLA <- atan2(data$latitude - la_coords[2], data$longitude - la_coords[1])
data$dirToSF <- atan2(data$latitude - sf_coords[2], data$longitude - sf_coords[1])

# Encode directions using dsin and cos
data$cosDirToLA <- cos(data$dirToLA)
data$sinDirToLA <- sin(data$dirToLA)

data$cosDirToSF <- cos(data$dirToSF)
data$sinDirToSF <- sin(data$dirToSF)

# Remove intermediate angle variables
data$dirToLA <- NULL
data$dirToSF <- NULL

data$cityProximityScore <- 1 / (1 + data$distToLA) + 1 / (1 + data$distToSF)
```

### 2.4.3 Best subset selection with new predictors and interaction terms

New function - Must run to run next chunk

```{r}
library(leaps)
#since can't call predict for regsubsets. doing the same thing as the for loop
predict.regsubsets=function(object,newdata,id,...){ #... allows for other arguments to be passed into the function
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

```

```{r, echo=FALSE}

library(car)
# k-fold CV


lm.fit <- lm(medianHouseValue~.-id-oceanProximity-aveBedrooms-aveRooms-population+bedroomsPerRoom+longitude:latitude+log(population)+I(medianIncome^2)+I(medianIncome^3)+I(housingMedianAge^2)+medianIncome:log(population)+bedroomsPerRoom:medianIncome+medianIncome:housingMedianAge+distToCenter:medianIncome+latitude:incomePerRoom+bedroomsPerRoom:distToLA+I(medianIncome/housingMedianAge)+I(latitude^2)+I(longitude^2)+I(latitude^3)+I(longitude^3)+log(incomePerRoom)+log(medianIncome/distToCenter)+housingMedianAge:incomePerRoom+cityProximityScore:housingMedianAge+cityProximityScore:bedroomsPerRoom+medianIncome:cosDirToLA+medianIncome:sinDirToLA+medianIncome:cosDirToSF+medianIncome:sinDirToSF,data = data)
summary(lm.fit)
nvmax <- length(coef(lm.fit)) - 1


k=10
set.seed(3)
folds=sample(1:k,nrow(data),replace=TRUE)
cv.errors=matrix(NA,k,nvmax, dimnames=list(NULL, paste(1:nvmax))) # NA means no data, NULL means no row names


for(j in 1:k){
  best.fit=regsubsets(medianHouseValue~.-id-oceanProximity-aveBedrooms-aveRooms-population+bedroomsPerRoom+longitude:latitude+log(population)+I(medianIncome^2)+I(medianIncome^3)+I(housingMedianAge^2)+medianIncome:log(population)+bedroomsPerRoom:medianIncome+medianIncome:housingMedianAge+distToCenter:medianIncome+latitude:incomePerRoom+bedroomsPerRoom:distToLA+I(medianIncome/housingMedianAge)+I(latitude^2)+I(longitude^2)+I(latitude^3)+I(longitude^3)+log(incomePerRoom)+log(medianIncome/distToCenter)+housingMedianAge:incomePerRoom+cityProximityScore:housingMedianAge+cityProximityScore:bedroomsPerRoom+medianIncome:cosDirToLA+medianIncome:sinDirToLA+medianIncome:cosDirToSF+medianIncome:sinDirToSF,data = data[folds!=j,],nvmax=nvmax)
  for(i in 1:nvmax){
    pred=predict(best.fit,data[folds==j,],id=i)
    cv.errors[j,i]=mean( (data$medianHouseValue[folds==j]-pred)^2)
  }
}
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
best.model.size <- as.numeric(names(which.min(mean.cv.errors)))
plot(mean.cv.errors,type='b')
points(best.model.size, mean.cv.errors[best.model.size], col = "red", pch = 19, cex = 1.5)
# Obtain the best subset model using the full data and CV selected id
reg.best=regsubsets(medianHouseValue~.-id-oceanProximity-aveBedrooms-aveRooms-population+bedroomsPerRoom+longitude:latitude+log(population)+I(medianIncome^2)+I(medianIncome^3)+I(housingMedianAge^2)+medianIncome:log(population)+bedroomsPerRoom:medianIncome+medianIncome:housingMedianAge+distToCenter:medianIncome+latitude:incomePerRoom+bedroomsPerRoom:distToLA+I(medianIncome/housingMedianAge)+I(latitude^2)+I(longitude^2)+I(latitude^3)+I(longitude^3)+log(incomePerRoom)+log(medianIncome/distToCenter)+housingMedianAge:incomePerRoom+cityProximityScore:housingMedianAge+cityProximityScore:bedroomsPerRoom+medianIncome:cosDirToLA+medianIncome:sinDirToLA+medianIncome:cosDirToSF+medianIncome:sinDirToSF,data = data, nvmax=nvmax)
coef(reg.best,best.model.size)


best.predictors = names(coef(reg.best, best.model.size))[-1]  # remove intercept
# Create formula dynamically
formula.best = as.formula(paste("medianHouseValue ~", paste(best.predictors, collapse = " + ")))

# Fit the model using lm
model.best = lm(formula.best, data = data)
summary(model.best)
vif(model.best,type = "predictor")
regfit <- regsubsets(formula.best,data = data,nvmax =length(coef(model.best)))
```

### 2.4.3 Adding in oceanProximity to model

```{r}
# With oceanProximity
# best so far
#oceanProximity+oceanProximity:medianIncome+housingMedianAge:oceanProximity+log(population):oceanProximity+latitude
new.formula = update(formula.best, . ~ . + oceanProximity + oceanProximity:distToCenter +oceanProximity:medianIncome)
model.best = lm(new.formula, data = data)

summary(model.best)
#vif(model.best,type = "predictor")


#with oceanProximity is better

#plot(model.best)

names(coef(model.best))
# Remove outliers
#data_unlev <- data[c(7419),] # these are all island - ridge or lasso can get rid of this

#data[data$oceanProximity == "ISLAND",]
#regfit.full=regsubsets(new.formula,data = data,nvmax =length(coef(model.best)))
#summary(regfit.full)
```

### 2.4.4 Ridge Regression

```{r}

library(glmnet)
x = model.matrix(new.formula, data = data)[, -1]  # Remove intercept column
y = data$medianHouseValue

# Perform 10-fold cross-validation to find the best lambda
set.seed(7)
lambda_grid <- 10^seq(-5, -2, length = 100)  # from 1e-5 to 1e-2
cv.ridge = cv.glmnet(x, y, alpha = 0,lambda = lambda_grid, nfolds = 10,standardize = TRUE)

# Extract best lambda (lambda.min gives lambda with minimum MSE)
best.lambda = cv.ridge$lambda.min

ridge.model = glmnet(x, y, alpha = 0, lambda = best.lambda)
coef(ridge.model)




```

### 2.5 Most significant predictors

# 3 Assessing the model performance

# 4 A Prediction Competition

## 4.1 Test MSE for the 3 main models

```{r}

test_data <- read.csv("Assignt1_test full.csv")
test_data$bedroomsPerRoom <- test_data $aveBedrooms / test_data $aveRooms
test_data$distToCenter <- sqrt((test_data$latitude - center_lat)^2 + (test_data$longitude - center_lon)^2)
test_data$incomePerRoom = test_data$medianIncome / test_data$aveRoom
# Add distance to LA
test_data$distToLA <- apply(test_data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000  # convert meters to km
})

#Add distance to SF
test_data$distToSF <- apply(test_data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
})

# Compute direction angles
test_data$dirToLA <- atan2(test_data$latitude - la_coords[2], test_data$longitude - la_coords[1])
test_data$dirToSF <- atan2(test_data$latitude - sf_coords[2], test_data$longitude - sf_coords[1])


# Encode directions using sin and cos
test_data$cosDirToLA <- cos(test_data$dirToLA)
test_data$sinDirToLA <- sin(test_data$dirToLA)

test_data$cosDirToSF <- cos(test_data$dirToSF)
test_data$sinDirToSF <- sin(test_data$dirToSF)

# Remove intermediate angle variables
test_data$dirToLA <- NULL
test_data$dirToSF <- NULL

test_data$cityProximityScore <- 1 / (1 + test_data$distToLA) + 1 / (1 + test_data$distToSF)
test_data$incomeFlag <- ifelse(test_data$medianIncome == 15.0001, 1, 0)

actual <- test_data$medianHouseValue


pred <- pmax(pmin(pred,500001),14999)
#OG
pred <- predict(initial_fit, newdata = test_data  )
pred <- pmax(pmin(pred,500001),14999)
og_mse <- mean((pred - actual)^2)
og_mse


#Best model
pred <- predict(model.best, newdata = test_data  )
pred <- pmax(pmin(pred,500001),14999)
best_mse <- mean((pred - actual)^2)
best_mse


#ridge
test_matrix <- model.matrix(new.formula, data = test_data)[, -1]
pred <- predict(ridge.model, s = best.lambda, newx = test_matrix)
pred <- pmax(pmin(pred,500001),14999)
ridge_mse <- mean((pred - actual)^2)
ridge_mse


```

best so far

[1] 4548677455

[1] 3763439107

[1] 3856612972

## 4.2 Chosen model and test MSE
